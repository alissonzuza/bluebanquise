#jinja2: lstrip_blocks: True
#### Blue Banquise file ####
## {{ansible_managed}}

# Documentation:
# https://slurm.schedmd.com/slurm.conf.html

## Very minimal slurm.conf

## Controller
ClusterName={{ slurm_cluster_name }}
ControlMachine={{ slurm_control_machine }}

{% if slurm_computes_config == "configless" %}
## Use configless mode
SlurmctldParameters=enable_configless
{% endif %}

## Authentication
SlurmUser=slurm
AuthType=auth/munge
CryptoType=crypto/munge

## Ports
SrunPortRange={{ slurm_srunportrange }}

## Files path
StateSaveLocation={{ slurm_statesavelocation }}
SlurmdSpoolDir={{ slurm_slurmdspooldir }}
SlurmctldPidFile={{ slurm_slurmctldpidfile }}
SlurmdPidFile={{ slurm_slurmdpidfile }}

## Logging
SlurmctldDebug={{ slurm_slurmctlddebug }}
SlurmctldLogFile={{ slurm_slurmctldlogfile }}
SlurmdDebug={{ slurm_slurmddebug }}
SlurmdLogFile={{ slurm_slurmdlogfile }}

## Timeouts
SlurmctldTimeout={{ slurm_slurmctldtimeout }}
SlurmdTimeout={{ slurm_slurmdtimeout }}

## We don't want a node to go back in pool without sys admin acknowledgement
ReturnToService={{ slurm_returntoservice }}

## Using pmi/pmi2/pmix interface for MPI
MpiDefault={{ slurm_mpidefault | default('',true) }}

## Basic scheduling based on nodes
SchedulerType={{ slurm_schedulertype }}
SelectType={{ slurm_selecttype }}

{% if slurm_grestypes is defined and slurm_grestypes is not none %}
# Gres configuration
GresTypes={{ slurm_grestypes }}
ProctrackType=proctrack/cgroup
TaskPlugin=affinity,cgroup
SelectTypeParameters=CR_Core_Memory,CR_CORE_DEFAULT_DIST_BLOCK,CR_ONE_TASK_PER_CORE

{% endif %}
{% if slurm_enable_accounting %}
## Accounting
AccountingStorageHost={{ slurm_control_machine }}
AccountingStorageType=accounting_storage/slurmdbd
JobAcctGatherType=jobacct_gather/linux
{% endif %}

## Acct Gather
AcctGatherNodeFreq={{ slurm_acct_gather_node_freq }}
AcctGatherEnergyType=acct_gather_energy/{{ slurm_acct_gather_energy_type }}
AcctGatherInterconnectType=acct_gather_interconnect/{{slurm_acct_gather_interconnect_type}}
AcctGatherFilesystemType=acct_gather_filesystem/{{ slurm_acct_gather_filesystem_type }}
AcctGatherProfileType=acct_gather_profile/{{ slurm_acct_gather_profile_type }}

## Additional content if exist
{% if slurm_slurm_conf_additional_content is defined and slurm_slurm_conf_additional_content is iterable %}
  {% for slurm_slurm_conf_content in slurm_slurm_conf_additional_content %}
{{ slurm_slurm_conf_content | default('', true) }}                                                                                                                       
  {% endfor %}
{% endif %}

## Nodes and partitions list
# Note: in configless mode, only slurm.conf is shipped to slurmds.
# We need to compact all configuration into this single file.
# -> no includes allowed.

{# NODENAMES #}
{# First we create a full list of all nodes that should be included in the file #}
{% set slurm_nodes = [] %}
{% for partition in slurm_partitions_list %}
  {% if partition['computes_groups'] is defined and partition['computes_groups'] is not none %}
    {% for compute_group in partition['computes_groups'] %}
      {% set slurm_nodes = (slurm_nodes + groups[compute_group] | unique) %}
    {% endfor %}
  {% endif %}
{% endfor %}
{# Now we use bb_nodes_profiles previously cached to pack these nodes per hardware groups #}
{%- set hw_packs = {} -%}
{% for node in slurm_nodes %}
  {% if bb_nodes_profiles[node] is defined %}
    {% if bb_nodes_profiles[node]['hw'] not in hw_packs %}
      {% do hw_packs.update({bb_nodes_profiles[node]['hw']: {'nodes': [], 'hw_specs': hostvars[node]['hw_specs']}}) %}
    {% endif %}
{{ hw_packs[bb_nodes_profiles[node]['hw']]['nodes'].append(node) }}
  {% else %}
# WARNING: node {{ node }} is associated with an hw and an os profile
  {% endif %}
{% endfor %}
{# Now we can generate final NodeNames for slurm #}
{% for hw_pack, hw_pack_vars in hw_pack.items() %}
  {% set buffer = hw_pack_vars['hw_specs'] %}
  {% if buffer.cpu.sockets is defined and buffer.cpu.sockets is not none and buffer.cpu.corespersocket is defined and buffer.cpu.corespersocket is not none and buffer.cpu.threadspercore is defined and buffer.cpu.threadspercore is not none%}
    {% set nodes_parameters = ' Sockets=' + (buffer.cpu.sockets|string) + ' CoresPerSocket=' + (buffer.cpu.corespersocket|string) + ' ThreadsPerCore=' + (buffer.cpu.threadspercore|string) %}
  {% else %}{# Will fail if cores is also not set. Expected minimal value. #}
    {% set nodes_parameters = ' Procs=' + (buffer.cpu.cores | string) %}
  {% endif %}
  {% if buffer.memory is defined and buffer.memory is not none %}
      {% set nodes_parameters = nodes_parameters + ' RealMemory=' + (buffer.memory|string) %}
  {% endif %}
  {% if buffer.slurm_extra_nodes_parameters is defined and buffer.slurm_extra_nodes_parameters is not none %}
      {% set nodes_parameters = nodes_parameters + ' ' +  buffer.slurm_extra_nodes_parameters %}
  {% endif %}
NodeName={{ hw_pack_vars['nodes'] | bluebanquise.commons.nodeset }} {{ nodes_parameters }}
{% endfor %}

{# PARTITIONS #}
## Partitions list
{% if slurm_partitions_list is defined and slurm_partitions_list %}
  {% for partition in slurm_partitions_list %}
    {% set partition_nodes = [] %}
    {% if partition['computes_groups'] is defined and partition['computes_groups'] is not none %}
      {% for compute_group in partition['computes_groups'] %}
        {% set slurm_nodes = (slurm_nodes + groups[compute_group] | unique) %}
      {% endfor %}
    {% endif %}
PartitionName={{partition.partition_name}} {% for arg, arg_value in partition.partition_configuration.items() %} {{ arg }}={{ arg_value }}{% endfor %} Nodes={{ partition_nodes | bluebanquise.commons.nodeset }}

  {% endfor %}
{% endif %}

{% if slurm_all_partition.enable is defined and slurm_all_partition.enable %}
## Partition for all
PartitionName=all {% for arg, arg_value in slurm_all_partition.partition_configuration.items() %} {{ arg }}={{ arg_value }}{% endfor %} Nodes={{ slurm_nodes | bluebanquise.commons.nodeset }}

{% endif %}
